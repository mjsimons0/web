<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c6{-webkit-text-decoration-skip:none;color:#000000;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-style:normal}.c13{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c2{color:#000000;text-decoration:none;vertical-align:baseline;font-style:normal}.c17{font-size:17pt;font-family:"Times New Roman";font-weight:700}.c16{font-weight:400;font-size:13pt;font-family:"Times New Roman"}.c1{font-size:12pt;font-family:"Times New Roman";font-weight:400}.c9{font-weight:400;font-size:14pt;font-family:"Times New Roman"}.c15{font-weight:400;font-size:16pt;font-family:"Times New Roman"}.c14{font-weight:400;font-size:12pt;font-family:"Arial"}.c11{-webkit-text-decoration-skip:none;text-decoration:underline;text-decoration-skip-ink:none}.c12{color:#000000;text-decoration:none;font-style:normal}.c18{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c10{font-weight:700;font-size:16pt;font-family:"Times New Roman"}.c5{color:#000000;text-decoration:none;vertical-align:baseline}.c8{font-weight:700;font-size:12pt;font-family:"Times New Roman"}.c3{height:11pt}.c7{vertical-align:sub}.c4{font-style:italic}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body style="text-align: center;"><div><p class="c0 c3"><span class="c2 c14"></span></p></div><p class="c13"><span class="c17">Exploration of Q-learning &amp; SARSA - Michael Simons</span></p><p class="c0"><span class="c2 c10">Introduction</span></p><p class="c0"><span class="c1">This report aims to observe the performance of the Q-learning algorithm and SARSA for both deterministic and stochastic versions of a simple environment, as well as a stock-trading environment. The supplemental code documents the process taken to create this environment, as well as the implementation of the Q-learning agent.</span></p><p class="c0 c3"><span class="c2 c9"></span></p><p class="c0"><span class="c2 c10">Part 1: Environment Formulation</span></p><p class="c0"><span class="c1">To train and evaluate the performance of our Q-agent, it is first necessary to produce an environment which the agent can interact with. For the purposes of this report, a simple 4x4 (16 states) grid-style environment was designed, allowing the agent four possible actions: </span><span class="c1 c4">DOWN, UP, RIGHT, LEFT</span><span class="c2 c1">.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c2 c1">The agent always begins in the upper-left corner of the grid, and its objective is to maximize its cumulative reward from the 15 maximally allowed timesteps. The mechanism in which rewards are computed is dependent on the environment type, which is deterministic or stochastic depending on which type was selected when the environment was initialized.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c2 c1">In the deterministic version of the environment, the reward is determined by the product of both of the agent&#39;s x and y coordinates. However, if the agent selects an action which would bring it out of boundaries, the agent is given a reward of -10 and does not move.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c2 c1">Similarly, the stochastic environment utilizes the same mechanism but additionally includes a randomly generated component within the range [-4,4] for each valid action taken.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c2 c1">Furthermore, maximizing the agent&#39;s reward should include reaching the goal, which is located in the bottom-right corner of the grid. Reaching this tile nets the agent a reward of 3500 divided by the timestep in which the goal was reached.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c2 c1">Prior to utilizing the environment, it is important to address safety considerations. In both versions of the environment (deterministic, stochastic), the agent is only able to select a fixed amount (4) actions, each of which are allowed within the defined environment. As previously mentioned, selecting an action which would bring it outside the boundaries of the grid i.e the agent selects action UP from position (0,0), is punished with a penalty of -10 and the agent remains stationary. This should stop the agent from leaving the defined 4x4 state space, resulting in the agent adhering to the intended secure behavior of the defined environment. To ensure that the environment&#39;s behavior is reliable and the implementation is correct, a test agent should first be implemented to observe the intended behavior.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c10">Random Agent</span></p><p class="c0"><span class="c2 c1">To confirm that the environment was successfully formulated, an agent with a random policy was implemented and run in both environment types. A sample episode from each environment type will now be displayed:</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c6 c1">Deterministic Environment (random agent):</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 151.11px; height: 161.98px;"><img alt="" src="static/image31.png" style="width: 151.11px; height: 161.98px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 152.50px; height: 161.30px;"><img alt="" src="static/image46.png" style="width: 152.50px; height: 161.30px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 150.50px; height: 159.30px;"><img alt="" src="static/image61.png" style="width: 150.50px; height: 159.30px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 150.50px; height: 160.67px;"><img alt="" src="static/image30.png" style="width: 150.50px; height: 160.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 153.50px; height: 165.78px;"><img alt="" src="static/image23.png" style="width: 153.50px; height: 165.78px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 150.15px; height: 162.50px;"><img alt="" src="static/image56.png" style="width: 150.15px; height: 162.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 148.96px; height: 160.72px;"><img alt="" src="static/image27.png" style="width: 148.96px; height: 160.72px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 148.50px; height: 159.54px;"><img alt="" src="static/image14.png" style="width: 148.50px; height: 159.54px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 150.99px; height: 157.72px;"><img alt="" src="static/image62.png" style="width: 150.99px; height: 157.72px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 145.50px; height: 153.36px;"><img alt="" src="static/image4.png" style="width: 145.50px; height: 153.36px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 142.50px; height: 152.98px;"><img alt="" src="static/image10.png" style="width: 142.50px; height: 152.98px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 146.50px; height: 155.85px;"><img alt="" src="static/image63.png" style="width: 146.50px; height: 155.85px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 145.50px; height: 155.67px;"><img alt="" src="static/image39.png" style="width: 145.50px; height: 155.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 146.94px; height: 154.72px;"><img alt="" src="static/image57.png" style="width: 146.94px; height: 154.72px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 144.35px; height: 154.94px;"><img alt="" src="static/image28.png" style="width: 144.35px; height: 154.94px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c2 c1">Total reward from episode: -31</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c6 c1">Stochastic Environment (random agent):</span></p><p class="c0"><span class="c2 c1">Total reward from episode: -14.4259436790219</span></p><p class="c0 c3"><span class="c6 c1"></span></p><p class="c0"><span class="c2 c10">Part 2: Implementing and Evaluating Tabular Methods</span></p><p class="c0 c3"><span class="c2 c10"></span></p><p class="c0"><span class="c2 c10">Q-Learning Agent</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c2 c1">The Q-learning agent will update Q-values for state-action pairs using a Q-table. The Q-update function is based on the previous Q-value, the immediate reward, highest possible reward to be taken in the next state (off-policy).</span></p><p class="c0"><span class="c1 c5 c4">Q-Learning Update function: </span></p><p class="c0"><span class="c1">Q(S</span><span class="c1 c7">t </span><span class="c1">, A</span><span class="c1 c7">t</span><span class="c1">) &larr; Q(S</span><span class="c1 c7">t </span><span class="c1">, A</span><span class="c1 c7">t </span><span class="c1">) + &alpha; &sdot; [R</span><span class="c1 c7">t+1</span><span class="c1">&nbsp;+ &gamma; &sdot; max</span><span class="c1 c7">a</span><span class="c1">Q(S</span><span class="c1 c7">t+1</span><span class="c1">&nbsp;, a) - Q(S</span><span class="c1 c7">t </span><span class="c1">, A</span><span class="c1 c7">t</span><span class="c2 c1">)], &nbsp;where </span></p><p class="c0"><span class="c1">S</span><span class="c1 c7">t</span><span class="c2 c1">&nbsp;= current state,</span></p><p class="c0"><span class="c1">A</span><span class="c1 c7">t</span><span class="c2 c1">&nbsp;= action taken in state</span></p><p class="c0"><span class="c1">R</span><span class="c1 c7">t+1</span><span class="c1">&nbsp;= immediate reward after taking action A</span><span class="c1 c7">t</span></p><p class="c0"><span class="c1">a = action which will be taken in state S</span><span class="c1 c12 c7">t+1</span></p><p class="c0"><span class="c1">&alpha; = learning rate</span></p><p class="c0"><span class="c2 c1">&gamma; = discount factor</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c2 c1">With the environment successfully formulated, a Q-agent was implemented to solve each version of the environment. As there are 16 states in the environment, and 4 possible actions at each state, the initial Q-table initializes to a 16x4 matrix of zeros, and updates after each timestep in training. The agent follows an epsilon-greedy based action selection, with an initial epsilon value of 1.0, successively decaying by a factor of 0.05 relative to its value at each episode, until 0.02 is reached. The learning rate was selected as a fixed value 0.2, and the discount factor a fixed value 0.9.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c2 c1">An agent was trained for 100 episodes in each of the deterministic and stochastic environments, and several visualizations were produced to evaluate the results:</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c6 c1">Deterministic Environment (q-learning):</span></p><p class="c0"><span class="c1 c5 c4">Training the agent:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 94.00px; height: 202.00px;"><img alt="" src="static/image1.png" style="width: 94.00px; height: 202.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 347.00px; height: 198.00px;"><img alt="" src="static/image8.png" style="width: 347.00px; height: 198.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 475.82px; height: 270.80px;"><img alt="" src="static/image41.png" style="width: 475.82px; height: 270.80px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c2 c1">The epsilon decay graph shows the successive decay at each episode in the training.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c1 c4">Evaluation of learned policy using greedy action selection:</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 270.00px; height: 181.00px;"><img alt="" src="static/image40.png" style="width: 270.00px; height: 181.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c2 c1">The agent&#39;s learned policy had a consistent reward each episode which is expected as the environment is deterministic. The performance of the learned policy was relatively high, earning a total reward of over 615 each episode.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c6 c1">Stochastic Environment (q-learning):</span></p><p class="c0 c3"><span class="c6 c1"></span></p><p class="c0"><span class="c1 c5 c4">Training the agent:</span></p><p class="c0 c3"><span class="c6 c1"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 94.00px; height: 202.00px;"><img alt="" src="static/image1.png" style="width: 94.00px; height: 202.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 348.00px; height: 197.00px;"><img alt="" src="static/image48.png" style="width: 348.00px; height: 197.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c3"><span class="c6 c1"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 472.96px; height: 272.20px;"><img alt="" src="static/image38.png" style="width: 472.96px; height: 272.20px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c3"><span class="c6 c1"></span></p><p class="c0 c3"><span class="c1 c5 c4"></span></p><p class="c0"><span class="c2 c1">Observably, the agent has successfully learned a policy which seems to consistently earn a total reward of 600 at each episode. The epsilon decay function also seems to work as intended. The learned policy will now be evaluated choosing the greedy action only.</span></p><p class="c0 c3"><span class="c1 c4 c5"></span></p><p class="c0 c3"><span class="c1 c5 c4"></span></p><p class="c0 c3"><span class="c1 c5 c4"></span></p><p class="c0"><span class="c1 c5 c4">Evaluation of learned policy using greedy action selection:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 280.50px; height: 194.85px;"><img alt="" src="static/image65.png" style="width: 280.50px; height: 194.85px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c2 c1">The performance of the learned policy is similar in value to the performance of the agent in the deterministic environment, earning over 600 in each episode. As expected, the reward in each episode varies as the environment is stochastic. The highest earned reward of the agent is over 625, while the lowest is under 605.</span></p><p class="c0 c3"><span class="c6 c1"></span></p><p class="c0"><span class="c10">Hyperparameter Re-evaluation</span></p><p class="c0 c3"><span class="c6 c1"></span></p><p class="c0"><span class="c2 c1">The performance of the Q-agent will now be improved by re-training with a variety of different parameter values.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c2 c8">Parameter 1: Learning rate</span></p><p class="c0 c3"><span class="c2 c8"></span></p><p class="c0"><span class="c2 c1">Firstly, we will observe the performance of the q-learning agent when trained with a higher learning rate. Previously, the agent&#39;s learning rate was a fixed value of 0.2. As the first of three experiments, the agent&#39;s learning rate will be increased to 0.4 and the results of each environment will be observed.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c6 c1">Deterministic Environment (lr = 0.4):</span></p><p class="c0 c3"><span class="c6 c1"></span></p><p class="c0"><span class="c1 c5 c4">Training the agent:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 94.00px; height: 202.00px;"><img alt="" src="static/image1.png" style="width: 94.00px; height: 202.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 308.00px; height: 198.00px;"><img alt="" src="static/image64.png" style="width: 308.00px; height: 198.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 498.00px; height: 291.00px;"><img alt="" src="static/image43.png" style="width: 498.00px; height: 291.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c2 c1">For the first 20 episodes in the deterministic environment, the agent appears to learn slower than when the learning rate was 0.2. However, the agent then seems to converge very quickly.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c1 c4">Evaluation of learned policy using greedy action selection:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 389.00px; height: 278.00px;"><img alt="" src="static/image15.png" style="width: 389.00px; height: 278.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c2 c1">The total rewards in each episode when evaluating this policy seems to be identical to the previously learned policy&#39;s results. This is expected since both of the learned policies solve the environment in the shortest amount of steps, and the environment is deterministic.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c1 c11">Stochastic Environment (lr = 0.4):</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c1 c4">Training the agent:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 94.00px; height: 202.00px;"><img alt="" src="static/image1.png" style="width: 94.00px; height: 202.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 346.00px; height: 198.00px;"><img alt="" src="static/image45.png" style="width: 346.00px; height: 198.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 487.50px; height: 284.38px;"><img alt="" src="static/image37.png" style="width: 487.50px; height: 284.38px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c2 c1">The agent with a learning rate of 0.4 seems to learn at a similar speed than with a learning rate of 0.2 in the stochastic environment.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c1 c5 c4">Evaluation of learned policy using greedy action selection:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 379.50px; height: 271.21px;"><img alt="" src="static/image59.png" style="width: 379.50px; height: 271.21px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c2 c1">The policy learned with a learning rate of 0.4 seems to have higher variance in the distribution, with a highest achieved total reward of over 630,and lowest total reward at nearly 595.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c2 c1">As the second experiment, the agent will now be trained with a lowered learning rate of 0.1 to observe the differences.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c6 c1">Deterministic Environment (lr = 0.1):</span></p><p class="c0 c3"><span class="c6 c1"></span></p><p class="c0"><span class="c1 c5 c4">Training the agent:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 94.00px; height: 202.00px;"><img alt="" src="static/image1.png" style="width: 94.00px; height: 202.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 349.00px; height: 198.00px;"><img alt="" src="static/image32.png" style="width: 349.00px; height: 198.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c3"><span class="c6 c1"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 498.00px; height: 291.00px;"><img alt="" src="static/image50.png" style="width: 498.00px; height: 291.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c2 c1">As expected due to the properties of the deterministic environment, the learning rate seemed to have little to no effect on the agent&#39;s training. While the Q-values are lower, the relativity remains.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c1 c4">Evaluation of learned policy using greedy action selection:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 389.00px; height: 278.00px;"><img alt="" src="static/image53.png" style="width: 389.00px; height: 278.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c2 c1">The total reward in the deterministic is just above 615 each episode, as expected.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c6 c1">Stochastic Environment (lr = 0.1):</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c1 c5 c4">Training the agent:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 94.00px; height: 202.00px;"><img alt="" src="static/image1.png" style="width: 94.00px; height: 202.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 347.00px; height: 195.00px;"><img alt="" src="static/image42.png" style="width: 347.00px; height: 195.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 498.00px; height: 291.00px;"><img alt="" src="static/image18.png" style="width: 498.00px; height: 291.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c1 c5 c4">Evaluation of learned policy using greedy action selection:</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 389.00px; height: 278.00px;"><img alt="" src="static/image36.png" style="width: 389.00px; height: 278.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c2 c1">The agent trained with the learning rate 0.1 seems to have performed the best in the stochastic environment, with a highest total award of over 635 and a lowest of about 605. </span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c2 c1">Since the learning rate seems to have little to no effect on the deterministic environment, yet lowering it seems to improve the agent&#39;s behavior in the stochastic environment, it will be lowered further.As a final experiment of the learning rate, the agent will be trained with a decreased rate of 0.05.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c6 c1">Deterministic Environment (lr = 0.05):</span></p><p class="c0 c3"><span class="c6 c1"></span></p><p class="c0"><span class="c1 c5 c4">Training the agent:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 94.00px; height: 202.00px;"><img alt="" src="static/image1.png" style="width: 94.00px; height: 202.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 349.00px; height: 195.00px;"><img alt="" src="static/image60.png" style="width: 349.00px; height: 195.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 498.00px; height: 291.00px;"><img alt="" src="static/image7.png" style="width: 498.00px; height: 291.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1 c5 c4">Evaluation of learned policy using greedy action selection:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 389.00px; height: 278.00px;"><img alt="" src="static/image15.png" style="width: 389.00px; height: 278.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c2 c1">As expected, the lowering of the learning rate seems to have no effect on the agent&#39;s training or performance in the deterministic environment.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c6 c1">Stochastic Environment (lr = 0.05):</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c1 c5 c4">Training the agent:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 94.00px; height: 202.00px;"><img alt="" src="static/image1.png" style="width: 94.00px; height: 202.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 347.00px; height: 196.00px;"><img alt="" src="static/image52.png" style="width: 347.00px; height: 196.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 498.00px; height: 291.00px;"><img alt="" src="static/image6.png" style="width: 498.00px; height: 291.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1 c5 c4">Evaluation of learned policy using greedy action selection:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 389.00px; height: 278.00px;"><img alt="" src="static/image55.png" style="width: 389.00px; height: 278.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c2 c1">The agent trained with a learning rate of 0.05 seems to perform slightly worse than the agent trained with a learning rate of 0.1, with a highest total reward of about 635 and a lowest total reward of about 600.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c8">Parameter 2: Number of Episodes</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c2 c1">The agent&#39;s performance will now be observed when trained with a variety of different episode numbers. For the purposes of these experiments, the original fixed learning rate of 0.2 will be utilized. Each of the previous models were trained for 100 episodes. In the first of three experiments, the model will be trained for 200 episodes. To account for epsilon decay, we will adjust the factor updating epsilon each episode from 0.95 to 0.97</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c6 c1">Deterministic Environment (episodes = 200, decay factor = 0.97):</span></p><p class="c0 c3"><span class="c6 c1"></span></p><p class="c0"><span class="c1 c5 c4">Training the agent:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 94.00px; height: 202.00px;"><img alt="" src="static/image1.png" style="width: 94.00px; height: 202.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 349.00px; height: 194.00px;"><img alt="" src="static/image20.png" style="width: 349.00px; height: 194.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 498.00px; height: 291.00px;"><img alt="" src="static/image17.png" style="width: 498.00px; height: 291.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1 c4">Evaluation of learned policy using greedy action selection:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 389.00px; height: 278.00px;"><img alt="" src="static/image15.png" style="width: 389.00px; height: 278.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c2 c1">The increase in episodes seems to have resulted in a similar performance as the original agent in the deterministic environment</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c6 c1">Stochastic Environment (episodes = 200, decay factor = 0.97):</span></p><p class="c0 c3"><span class="c6 c1"></span></p><p class="c0"><span class="c1 c5 c4">Training the agent:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 94.00px; height: 202.00px;"><img alt="" src="static/image1.png" style="width: 94.00px; height: 202.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 348.00px; height: 196.00px;"><img alt="" src="static/image16.png" style="width: 348.00px; height: 196.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 498.00px; height: 291.00px;"><img alt="" src="static/image3.png" style="width: 498.00px; height: 291.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1 c4">Evaluation of learned policy using greedy action selection:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 389.00px; height: 278.00px;"><img alt="" src="static/image29.png" style="width: 389.00px; height: 278.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c2 c1">Increasing the episodes to 200 seems to have slightly increased the variance of the performance in the stochastic environment, likely just by chance.</span></p><p class="c0"><span class="c2 c1">The second experiment will entail training the agent with an episode count of 60 and an epsilon factor of 0.94</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c6 c1">Deterministic Environment (episodes = 50, decay factor = 0.94):</span></p><p class="c0 c3"><span class="c6 c1"></span></p><p class="c0"><span class="c1 c5 c4">Training the agent:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 94.00px; height: 202.00px;"><img alt="" src="static/image1.png" style="width: 94.00px; height: 202.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 284.00px; height: 194.00px;"><img alt="" src="static/image19.png" style="width: 284.00px; height: 194.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 497.00px; height: 291.00px;"><img alt="" src="static/image12.png" style="width: 497.00px; height: 291.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1 c4">Evaluation of learned policy using greedy action selection:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 389.00px; height: 278.00px;"><img alt="" src="static/image15.png" style="width: 389.00px; height: 278.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c2 c1">The decrease in episodes also seems to have resulted in a similar performance as the original agent in the deterministic environment</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c6 c1">Stochastic Environment (episodes = 50, decay factor = 0.94):</span></p><p class="c0 c3"><span class="c6 c1"></span></p><p class="c0"><span class="c1 c5 c4">Training the agent:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 94.00px; height: 202.00px;"><img alt="" src="static/image1.png" style="width: 94.00px; height: 202.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 285.00px; height: 198.00px;"><img alt="" src="static/image58.png" style="width: 285.00px; height: 198.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 436.50px; height: 255.03px;"><img alt="" src="static/image5.png" style="width: 436.50px; height: 255.03px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1 c4">Evaluation of learned policy using greedy action selection:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 398.00px; height: 278.00px;"><img alt="" src="static/image2.png" style="width: 398.00px; height: 278.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c2 c1">Decreasing the episodes to 50 has decreased the performance of the agent in the stochastic environment drastically, with a highest reward of about 618 and a minimum of 600.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c1">In the final experiment of the episodes parameter, the episodes will be set to 150, and the epsilon decay factor to 0.96.</span></p><p class="c0 c3"><span class="c2 c10"></span></p><p class="c0"><span class="c6 c1">Deterministic Environment (episodes = 150, decay factor = 0.96):</span></p><p class="c0 c3"><span class="c6 c1"></span></p><p class="c0"><span class="c1 c5 c4">Training the agent:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 94.00px; height: 202.00px;"><img alt="" src="static/image1.png" style="width: 94.00px; height: 202.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 349.00px; height: 199.00px;"><img alt="" src="static/image35.png" style="width: 349.00px; height: 199.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 498.00px; height: 291.00px;"><img alt="" src="static/image13.png" style="width: 498.00px; height: 291.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1 c4">Evaluation of learned policy using greedy action selection:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 389.00px; height: 278.00px;"><img alt="" src="static/image53.png" style="width: 389.00px; height: 278.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c2 c1">The increase in episodes to 150 resulted in a similar performance as the original agent in the deterministic environment</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c6 c1">Stochastic Environment (episodes = 150, decay factor = 0.96):</span></p><p class="c0 c3"><span class="c6 c1"></span></p><p class="c0"><span class="c1 c5 c4">Training the agent:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 94.00px; height: 202.00px;"><img alt="" src="static/image1.png" style="width: 94.00px; height: 202.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 348.00px; height: 197.00px;"><img alt="" src="static/image21.png" style="width: 348.00px; height: 197.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 498.00px; height: 291.00px;"><img alt="" src="static/image51.png" style="width: 498.00px; height: 291.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1 c4">Evaluation of learned policy using greedy action selection:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 389.00px; height: 278.00px;"><img alt="" src="static/image33.png" style="width: 389.00px; height: 278.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c2 c1">Increasing the episodes to 150 has slightly decreased the highest performance of the agent in the stochastic environment drastically, with a highest reward of nearly 625 and a lowest of under 605.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c2 c1">Based on these experiments, the hyperparameters I would suggest would be a learning rate of 0.1 with 100 episodes and an epsilon decay factor of 0.95, as this combination seemed to have the highest performance among the experimented values.</span></p><p class="c0 c3"><span class="c2 c10"></span></p><p class="c0"><span class="c2 c10">SARSA Agent</span></p><p class="c0 c3"><span class="c2 c10"></span></p><p class="c0"><span class="c1">A final type of agent was implemented, following the SARSA algorithm. This algorithm&#39;s only difference from Q-learning is that it is on-policy, meaning the update of a Q-value is based on the action the policy will select in the next state rather than the action which will provide the highest reward.</span></p><p class="c0 c3"><span class="c2 c10"></span></p><p class="c0"><span class="c1 c5 c4">SARSA(State-Action-Reward-State-Action) Update function: </span></p><p class="c0"><span class="c1">Q(S</span><span class="c1 c7">t </span><span class="c1">, A</span><span class="c1 c7">t</span><span class="c1">) &larr; Q(S</span><span class="c1 c7">t </span><span class="c1">, A</span><span class="c1 c7">t </span><span class="c1">) + &alpha; &sdot; [R</span><span class="c1 c7">t+1</span><span class="c1">&nbsp;+ &gamma; &sdot; Q(S</span><span class="c1 c7">t+1</span><span class="c1">&nbsp;, A</span><span class="c1 c7">t+1</span><span class="c1">) - Q(S</span><span class="c1 c7">t </span><span class="c1">, A</span><span class="c1 c7">t</span><span class="c2 c1">)], &nbsp;where </span></p><p class="c0"><span class="c1">S</span><span class="c1 c7">t</span><span class="c2 c1">&nbsp;= current state</span></p><p class="c0"><span class="c1">A</span><span class="c1 c7">t</span><span class="c1">&nbsp;= action taken in state S</span><span class="c1 c12 c7">t</span></p><p class="c0"><span class="c1">R</span><span class="c1 c7">t+1</span><span class="c1">&nbsp;= immediate reward after taking action A</span><span class="c1 c7">t</span></p><p class="c0"><span class="c1">A</span><span class="c1 c7">t</span><span class="c1">&nbsp;= action which will be taken in state S</span><span class="c1 c7">t+1</span><span class="c2 c1">&nbsp;based on the policy.</span></p><p class="c0"><span class="c1">&alpha; = learning rate</span></p><p class="c0"><span class="c1">&gamma; = discount factor</span></p><p class="c0 c3"><span class="c2 c10"></span></p><p class="c0 c3"><span class="c2 c10"></span></p><p class="c0 c3"><span class="c2 c10"></span></p><p class="c0 c3"><span class="c2 c10"></span></p><p class="c0"><span class="c2 c1">The agent was trained in both environments and the results will now be displayed.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c6 c1">Deterministic Environment:</span></p><p class="c0 c3"><span class="c6 c1"></span></p><p class="c0"><span class="c1 c5 c4">Training the agent:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 94.00px; height: 202.00px;"><img alt="" src="static/image1.png" style="width: 94.00px; height: 202.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 355.50px; height: 200.89px;"><img alt="" src="static/image25.png" style="width: 355.50px; height: 200.89px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 455.50px; height: 266.17px;"><img alt="" src="static/image9.png" style="width: 455.50px; height: 266.17px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c2 c1">SARSA learned slightly slower than Q-learning in the deterministic environment, but seems to have found an effective strategy nonetheless.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c1 c5 c4">Evaluation of learned policy using greedy action selection:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 377.50px; height: 269.78px;"><img alt="" src="static/image15.png" style="width: 377.50px; height: 269.78px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c6 c1">Stochastic Environment:</span></p><p class="c0 c3"><span class="c6 c1"></span></p><p class="c0"><span class="c1 c5 c4">Training the agent:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 94.00px; height: 202.00px;"><img alt="" src="static/image1.png" style="width: 94.00px; height: 202.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 360.50px; height: 205.86px;"><img alt="" src="static/image49.png" style="width: 360.50px; height: 205.86px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 498.00px; height: 291.00px;"><img alt="" src="static/image24.png" style="width: 498.00px; height: 291.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1 c5 c4">Evaluation of learned policy using greedy action selection:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 340.50px; height: 243.21px;"><img alt="" src="static/image22.png" style="width: 340.50px; height: 243.21px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c2 c1">SARSA also proved to be effective in the stochastic environment, with a maximum reward of over 635 and a minimum of just under 609 in testing.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c10">Comparing Q-learning and SARSA performance</span></p><p class="c0 c3"><span class="c2 c15"></span></p><p class="c0"><span class="c2 c1">Each of the Q-learning and SARSA agents were trained using the same parameters. To evaluate the effectiveness of each method, a visualization was generated showing the total rewards for 40 episodes where each of the agents selected greedy actions only.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 286.94px; height: 204.20px;"><img alt="" src="static/image26.png" style="width: 286.94px; height: 204.20px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 288.50px; height: 206.66px;"><img alt="" src="static/image54.png" style="width: 288.50px; height: 206.66px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c2 c1">In the deterministic environment, there was an overlap in the total rewards from each agent since both of the agents solved the environment in the minimum number of timesteps, earning the maximum possible reward.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c2 c1">In the stochastic environment, the performance of each agent was quite similar. However, the SARSA agent appears to have out-performed the Q-learning agent slightly, with a higher average observed reward and a highest observed reward of nearly 640. These results indicate that SARSA agents may out-perform Q-learning agents in stochastic environments like the one defined, however it should be noted that this is not a thorough enough investigation since one set of hyperparameters was used. </span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c2 c1">Nonetheless, both Q-learning and SARSA both exhibited similar performances in the grid environment.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c10">Reward Function</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c2 c1">The development of aligning these agents with the environment was prolonged due to overlooked importances in developing the reward function. Adding the reward penalty of -10 for selecting an action which takes the agent out of bounds, for example, had a drastic effect on the ability of the agent to learn. This was initially overlooked, causing the agent to spend many episodes developing poor strategies. This one example highlights the importance of using a reward function that is aligned with your objective. Another aspect which was important in aligning the agent with the objective was defining the reward as a product of the coordinates. Because the goal position is located in the bottom right corner, the reward increases as the agent gets closer, motivating it to go towards the goal position. Increasing the reward as the agent gets closer keeps the agent informed about its progress, which is another important aspect of reward function design which was highlighted through these experiments.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0 c3"><span class="c2 c10"></span></p><p class="c0"><span class="c11 c10">Part 3: Application of Q-Learning Agent to Stock Environment</span></p><p class="c0 c3"><span class="c2 c10"></span></p><p class="c0"><span class="c2 c1">The Q-Learning agent&#39;s capability will now be tested in a stock trading environment, based on a dataset containing 504 entries containing relevant information about the historical stock price for Nvidia in the last 2 years, including opening price, intraday highs and lows, closing price, etc. the Q-learning agent trained for 30 episodes on the dataset considering the activity in the past 35 days. The visualizations related to the agent&#39;s training will now be shown.</span></p><p class="c0 c3"><span class="c2 c1"></span></p><p class="c0"><span class="c1 c4">Training the agent</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 98.00px; height: 68.00px;"><img alt="" src="static/image44.png" style="width: 98.00px; height: 68.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 228.00px; height: 71.00px;"><img alt="" src="static/image34.png" style="width: 228.00px; height: 71.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 497.00px; height: 291.00px;"><img alt="" src="static/image11.png" style="width: 497.00px; height: 291.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c2 c1">The Q-learning agent successfully began to learn a successful trading strategy over the 30 episodes. To evaluate the learned policy, the agent was evaluated on testing data using greedy action selection.</span></p><p class="c0"><span class="c1 c4">Evaluation of learned policy using greedy action selection:</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 430.50px; height: 297.89px;"><img alt="" src="static/image47.png" style="width: 430.50px; height: 297.89px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c2 c1">Total account value gained over 65 days: 924.64</span></p><p class="c0"><span class="c2 c1">The agent successfully earned a significant reward on the testing data, indicating successful learning.</span></p></body></html>